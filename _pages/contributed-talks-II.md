---
layout: page
permalink: /contributed-talks-II/
title: Contributed talks on Wednesday, 13/09
description: 
nav: false
---

### Deep learning (Room HS01)

<table>
<colgroup>
<col width="15%" />
<col width="85%" />
</colgroup>
<thead>
<tr class="header">
<th>Time</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td markdown="span">13:30–13:50</td>
<td markdown="span">_Nina Ma_   

**Mastering prompt engineering: Unleashing the potential of language models**   

Prompt engineering is a crucial aspect of harnessing the power of language models, enabling tailored and effective interactions. This talk delves into the art of creating prompts that yield desired outputs from language models like GPT-3.5. Prompt engineering involves using instructional completion prompts and fine-tuning techniques to enhance model performance. Completion prompts guide the model's responses, while fine-tuning optimizes its behavior. By leveraging fine-tuning, you can expose the model to a broader range of training data than can fit in a prompt, thereby achieving token savings and lower latency rates. Various strategies make prompt engineering more effective. Initiating with clear instructions, specificity, and tone-setting helps frame the context. Breaking tasks into sub-tasks and using probing questions elaborates outcomes effectively. Refining the prompt iteratively and employing additional tactics like delimiters, structured output requests, and temperature parameter modifications further enhance results. Prompt engineering also encompasses zero-shot, one-shot, and few-shot approaches. These methods leverage task descriptions, examples, and fine-tuning, respectively, to make language models more adaptable to specific tasks. However, prompt engineering isn't without challenges. Concepts like prompt leaking, prompt injection, and jailbreaking highlight potential risks associated with unintended model behavior and data privacy. In a world where language models play an increasingly significant role, mastering prompt engineering empowers us to extract meaningful and tailored insights, responses, and content from these powerful AI tools.

</td>
</tr>
<tr>
<td markdown="span">13:50–14:10</td>
<td markdown="span"> _Emanuele Sansone_   

**GEDI: GEnerative and DIscriminative training for self-supervised learning**   
Self-supervised learning is a popular and powerful method for utilizing large amounts of unlabeled data, for which a wide variety of training objectives have been proposed in the literature. In this talk, we provide a Bayesian analysis of state-of-the-art self-supervised learning objectives and propose a unified formulation based on likelihood learning. Our analysis suggests a simple method for integrating self-supervised learning with generative models, allowing for the joint training of these two seemingly distinct approaches. We refer to this combined framework as GEDI, which stands for GEnerative and DIscriminative training. Additionally, we demonstrate an instantiation of the GEDI framework by integrating an energy-based model with a cluster-based self-supervised learning model. Through experiments on synthetic and real-world data, including SVHN, CIFAR10, and CIFAR100, we show that GEDI outperforms existing self-supervised learning strategies in terms of clustering performance by a wide margin. We also showcase an application to the neuro-symbolic setting, where GEDI can learn symbolic representations supporting learning and reasoning in the small data regime without the need for additional supervision or costly pre-training steps.

</td>
</tr>
<tr>
<td markdown="span">14:10–14:30</td>
<td markdown="span">_Tobias Hoffmann_   

**Local syntactic coherence effects in GPT3 surprisals**   

Local syntactic coherence (LSC) effects have shown that the human sentence processor (HSP) can be misguided by a locally embedded sequence of words that could form a sentence in isolation, but must be analyzed differently in the left context of the sentence. These effects have been attributed to temporal local affixes (SOPARS, Tabor_et_al. 2004) and word-wise prediction in recurrent networks (Konieczny_et_al. 2005), among others. Large language models, such as GPT-3+, have impressive capabilities in language comprehension and production. Due to their transformer-based architecture, they should be immune to LSCs. We used the OpenAI API to retrieve the surprisal values for our test items word by word. We then reanalyzed our eye-tracking reading data on sentences with local syntactic coherence embedded in short contexts (citation omitted). Contexts were constructed to draw attention to either the local coherence meaning or the global meaning of the target sentence. While the context manipulation affected the size of the LSC effect, it did not alter GPT-3 surprises in the critical region. While GPT-3 surprise scores were good predictors of total reading time, they did not eliminate LSC effects. We conclude that HSP, unlike transformer-based LLMs, employs mechanisms of local attention.
</td>
</tr>
</tbody>
</table>
<br>

### Perception (Room SR003)

<table>
<colgroup>
<col width="15%" />
<col width="85%" />
</colgroup>
<thead>
<tr class="header">
<th>Time</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td markdown="span">13:30–13:50</td>
<td markdown="span">_Tin Mišić_   

**Object tracking using an active stereo visual system**   

Object tracking has many applications in robotics, autonomous vehicles, and surveillance. Among all methods, those that use active stereo systems with moving cameras stand out the most. Special emphasis in this paper is given to approaches of active stereo systems that use virtual horopters and log-polar image mapping. An active stereo system is designed that uses ordinary web cameras and servo motors. The cameras and motors are connected with plastic parts printed on a 3D printer, and the entire system runs on a personal computer. The system's real-time performance, ability to track a moving object against various backgrounds, consistency of focus on the object, and accuracy of the estimated object position were tested. Approaches using Cartesian image mapping and those using log-polar mapping were compared. Based on the obtained results, some advantages of log-polar mapping over Cartesian mapping were shown, as well as the drawbacks of the specific implementation.

</td>
</tr>
<tr>
<td markdown="span">13:50–14:10</td>
<td markdown="span">_Anna Ermolaeva_  

**Visual perception and the brain. Investigating the effectiveness of the visual approach in teaching and learning settings.**  
This research explores the effectiveness of integrating visual approaches in education, investigating how visual perception and cognitive processes interact to improve learning outcomes. Rooted in cognitive science and educational psychology, the study aims to enhance learning by strategically employing visual aids. The introduction highlights the role of visual perception in learning and outlines the study's goal to assess the impact of visual approaches. The literature review surveys studies and theories on visual perception and learning, showcasing the positive effects of visual aids on comprehension and memory. The methodology details the research design, target group, visual aids, and assessment methods. Data collection involves teaching sessions with visual aids for the experimental group and conventional methods for the control group. Results are analyzed quantitatively and qualitatively to understand variations in performance and engagement. The discussion interprets outcomes and their implications for teaching practices. Recommendations offer practical guidance for educators to integrate visual aids effectively. This research contributes insights for optimizing teaching methods and enhancing learning experiences.
</td>
</tr>
<tr>
<td markdown="span">14:10–14:30</td>
<td markdown="span">_Niranjan Rajesh_  

**Investigating brain-like CNNs and their consequences**   

​​Recent research in Neuro-inspired Machine Learning in the visual domain has resulted in CNNs that are modelled after the primate visual systems. These are usually done through structural or representational alignment of the CNNs with their primate brain counterparts. These models claim to be behaviourally comparable to human visual intelligence and correlations were found in areas like robustness to visual adversarial attacks. My work will further investigate this consequence of primate visual system alignment in CNN i.e. the connection between brain likeness and adversarial robustness.
</td>
</tr>
</tbody>
</table>


